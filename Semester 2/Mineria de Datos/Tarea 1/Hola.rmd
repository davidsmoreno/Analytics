---
title: "Tarea 1 Mineria de Datos"
author: "David Romero, Luisa De La Horita, David Moreno"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: false  # Set to false to disable section numbering
    theme: cosmo
    highlight: tango
    code_folding: show
---



```{r,echo=FALSE}
library(dplyr)      # For data manipulation
library(ggplot2)    # For data visualization
library(tidyr)      # For data tidying
library(readr)      # For reading data
library(readxl)     # For reading excel files
library(psych)      # For descriptive statistics
library(zoo)        # For handling missing values
library(corrplot)   # Correlation Matrix
library(lubridate)  #Extract Hour
library(gridExtra)

```




# Punto 1
Primero, vamos a unir las bases de datos, identificando y eliminando duplicados basados en las fechas para conservar únicamente el primer registro. Luego, aplicaremos una técnica de imputación (asignación o eliminación) para manejar los datos faltantes.

```{r, warning=FALSE}
# Read Inflow Data
weather <- read_excel("WeatherData_1.xlsx", skip = 1, col_names = c("Date", "Rainfall depth (mm)", "Air temperature (°C)", "Air humidity (%)", "Windspeed (km/h)"))

# Read inflow data
inflow <- read_excel("InflowData_1.xlsx", skip = 1, col_names = c("Date", "DMA A", "DMA B", "DMA C", "DMA D", "DMA E", "DMA F", "DMA G", "DMA H", "DMA I", "DMA J"))

# Convert Date column to POSIXct format
weather$Date <- as.POSIXct(weather$Date, format = "%d/%m/%Y %H:%M")
inflow$Date <- as.POSIXct(inflow$Date, format = "%d/%m/%Y %H:%M")

# Merge inflow and weather data frames by "Date" column, retaining all rows from inflow
df_data <- merge(inflow, weather, by = "Date", all.x = TRUE)

# Remove duplicate rows based on the Date column, keeping only the first occurrence
df_data <- df_data[!duplicated(df_data$Date, fromLast = TRUE), ]

```

Primero, procedemos a leer los dos conjuntos de datos y renombrar las columnas con nombres descriptivos. Para facilitar la unión de los conjuntos, utilizaremos "Date" como nombre común para la columna que representa la fecha.

Luego, convertiremos la columna "Date" al formato POSIXct para asegurarnos de que esté en el formato correcto, que es día/mes/año/hora/minuto.

Posteriormente, creamos la variable df_data y unimos las dos bases de datos mediante un left join de la base de datos "Inflow" con la base "weather".

### Procesamiento de los datos

```{r}
str(df_data)
```
Aqui vemos que los tipos de datos de las las columnas son númericos y de tipo chr que es string, acá vemos un problema y es que esta tomando el string "#N/A" no como vacio sino como string, las demás variables de DMA deberian ser númericas para esto vamos a hacer una tranformación a los datos 



```{r}
df_data <- df_data %>%
  mutate(
    # First operation: Handle character columns
    across(where(is.character), ~ {
      # Replace "#N/A" with NA, then extract numbers and convert to numeric
      as.numeric(gsub("[^0-9.]+", "", ifelse(. == "#N/A", NA, .)))
    }),
    # Second operation: Convert DMA columns to integer
    across(c(`DMA A`, `DMA B`, `DMA C`, `DMA D`, `DMA E`, `DMA F`, `DMA G`, `DMA H`, `DMA I`, `DMA J`), as.integer)
  )
str(df_data)
```

Aqui hacemos un pipeline indicando que remplace todos los strings de la forma "#NA" con NA de variable descriptiva en r, luego extraiga los numeros y los combierta a numerico, y luego convertimos toda las columnas en enteros.

### Tecnica de imputacion

Para este ejericio vamos a ver la cantidad de Na que tenemos en cada columns para ver que método utilizamos

```{r}
# Calculate NA proportions for each column
na_proportions <- colMeans(is.na(df_data))

# Create a data frame with column names and NA proportions
na_summary <- data.frame(
  Column = names(na_proportions),
  NA_Proportion = na_proportions
)

# Arrange the summary data frame by NA proportions in descending order
na_summary <- na_summary %>%
  arrange(desc(NA_Proportion))

# Print the summary
print(na_summary)

```

Observamos que el porcentaje más alto de valores vacíos es del 0.13%(DMA F), seguido por el 0.11%(DMA I) y el 0.10% de la columna DMA G, donde la cantidad de valores vacíos es más pronunciada. Para llevar a cabo esta actividad, procederemos a eliminar estos valores. Posteriormente, imputaremos los valores faltantes.
```{r }

#Primer analisis eliminar NA
df_data_1<-na.omit(df_data)

```

### Histogramas de las columnas


```{r, fig.width=12, fig.height=10}
# Identify numeric columns. This is a placeholder; replace it with your actual logic to identify numeric columns
numeric_cols <- sapply(df_data_1, is.numeric)

# Set up the plotting window to display multiple plots based on the number of numeric columns
par(mfrow=c(ceiling(sqrt(sum(numeric_cols))), ceiling(sqrt(sum(numeric_cols)))))

# Loop through each numeric column to create a histogram
for(col in names(df_data_1)[numeric_cols]) {
  hist(df_data_1[[col]], main = paste("Histogram of", col),
       xlab = col, ylab = "Frequency")
}

# Reset plotting window to default
par(mfrow=c(1, 1))

```
Aquí generamos un gráfico para visualizar las distribuciones de nuestras columnas y entender mejor la naturaleza de los datos de cada variable. Observamos que la variable "Rainfall depth" muestra una alta concentración de valores en cero, lo que sugiere que la información disponible para esta variable es limitada

Además, al analizar los datos en general, identificamos posibles valores atípicos ("outliers") (Para el Bono) en la columna DMA A, particularmente para valores mayores a 15. 



## Punto 1.2
Se desea identificar si se tiene un patrón dentro de las series de tiempo, para esto se debe usar la técnica de media móvil variando la ventada de tiempo que se usa para cada corrida. Se le indica que debe usar 5 ventanas de tiempo diferentes para compara los resultados y asi poder conluir sobre el patrón en cada serie de tiempo.


### Media Movil

Vamos a calcular la media movil para 5 periodos de tiempo diferentes, vamos a  hacerlo por horas, por meses, con ventanas de tiempo de 1 mes, 2 meses y 5 meses

#### Análisis de corrrelación

Primero vamos a hacer un análisis de correlación para ver que variables tienen una realción más fuerte y, por tanto, podrían ser más relevnates para un análisis más detallado, 


```{r, fig.width=10, fig.height=8}
numeric_df <- df_data %>%
  select(where(is.numeric))

# Calculate the correlation matrix for numeric columns
correlation_matrix <- cor(numeric_df, use = "complete.obs")

# Visualize the correlation matrix using corrplot
corrplot(correlation_matrix, 
         method = "color", 
         type = "upper", 
         order = "hclust",
         tl.col = "black", 
         tl.srt = 45, 
         addCoef.col = "black", 
         col = colorRampPalette(c("#6D9EC1", "white", "#E46726"))(200), 
         diag = FALSE)
```


Aqui en la gráfica vemos algo interesante, xiste una correlación positiva fuerte entre algunas de las variables DMA (por ejemplo, DMA F con DMA D, DMA H con DMA A y DMA E, etc.). Esto sugiere que hay una relación lineal directa entre estos pares de variables: cuando una aumenta, la otra también tiende a aumentar.
La temperatura del aire (Air temperature (°C)) también tiene correlaciones positivas fuertes con varias variables DMA, lo que podría indicar que el consumo de agua o la demanda en estas zonas está influenciada por la temperatura.


Correlaciones Negativas:

La velocidad del viento (Windspeed (km/h)) presenta una correlación negativa con la humedad del aire (Air humidity (%)). Esto sugiere que, en general, a medida que aumenta la velocidad del viento, disminuye la humedad en el aire.



```{r}
df_data_1$Date <- as.POSIXct(df_data_1$Date, format="%d/%m/%Y %H:%M")
df_data_1$Hour <- hour(df_data_1$Date)

hourly_averages <- df_data_1 %>%
  group_by(Hour) %>%
  summarise(Average_DMA_A = mean(`DMA A`, na.rm = TRUE))

# Creamos el gráfico con ggplot2
ggplot(hourly_averages, aes(x = Hour, y = Average_DMA_A)) +
  geom_line() +  # Línea que conecta los puntos medios
  geom_point() +  # Puntos que representan los promedios reales por hora
  labs(title = "Media Móvil por Hora para DMA A", x = "Hora del Día", y = "Media de DMA A") +
  scale_x_continuous(breaks = 0:23) +  # Aseguramos que el eje x tenga las 24 horas
  theme_minimal()  # Tema minimalista para el gráfico

```
Para esta gráfica vemos algo interesante: Para esta gráfica, observamos un patrón intrigante: DMA A comienza a incrementar a partir de las 6 de la mañana, alcanzando su punto máximo a las 9 de la mañana. Posteriormente, experimenta un ligero descenso hasta las 12 de la noche, momento en el cual disminuye de manera más notable. Este comportamiento se mantiene de manera constante a lo largo de las horas, lo cual sugiere un posible vínculo con la temperatura ambiental. Dado que durante las horas del día la presencia del sol es más intensa, esto podría incrementar la demanda de agua



```{r,fig.width=10, fig.height=8}

df_data_3 <- df_data_1 %>%
  mutate(Year = year(Date),
         Month = month(Date),
         YearMonth = as.yearmon(Date))

# Group by year and month to calculate the monthly average of DMA A
monthly_avg <- df_data_3 %>%
  group_by(YearMonth) %>%
  summarise(MonthlyAvg_DMA_A = mean(`DMA A`, na.rm = TRUE))

# Calculate the one-month moving average for the monthly averages of DMA A
monthly_avg$MovingAvg_1M <- rollapply(monthly_avg$MonthlyAvg_DMA_A, width = 1, FUN = mean, by.column = TRUE, fill = NA, align = 'right')

# Visualization with ggplot2

ggplot(monthly_avg, aes(x = YearMonth, y = MovingAvg_1M)) +
  geom_line() +
  geom_point() +
  labs(title = "1-Month Moving Average of Monthly Avg DMA A",
       x = "Year and Month",
       y = "1-Month Moving Average") +
  scale_x_yearmon(breaks = seq(min(monthly_avg$YearMonth), max(monthly_avg$YearMonth), by = 0.1)) +
  theme_minimal()

```

Aqui también vemos algo interesante y es que existe una mayor demanda de agua en DMA A durante las épocas más cálidas del año. Se registra un aumento en el consumo desde febrero hasta agosto, destacándose julio y agosto como los periodos con los niveles más altos de demanda. Este mismo patrón se repite también para el año 2022, lo que sugiere una correlación entre las estaciones climáticas y el consumo de agua.

```{r,warning=FALSE}
df_data_2 <- df_data_1 %>%
  mutate(YearMonth = format(Date, "%Y-%m")) %>%
  group_by(YearMonth) %>%
  summarise(Monthly_Avg_DMA_A = mean(`DMA A`, na.rm = TRUE)) %>%
  ungroup()

# Calculamos las medias móviles para cada uno de los periodos solicitados
# Change the names of the moving averages columns
df_data_2$MA_1 <- rollapply(df_data_2$Monthly_Avg_DMA_A, width = 1, FUN = mean, fill = NA, align = 'right')
df_data_2$MA_5 <- rollapply(df_data_2$Monthly_Avg_DMA_A, width = 2, FUN = mean, fill = NA, align = 'right')
df_data_2$MA_10 <- rollapply(df_data_2$Monthly_Avg_DMA_A, width = 5, FUN = mean, fill = NA, align = 'right')
df_data_2$MA_24 <- rollapply(df_data_2$Monthly_Avg_DMA_A, width = 10, FUN = mean, fill = NA, align = 'right')

# Convert YearMonth back to a date object for visualization
df_data_2$YearMonth <- as.Date(paste0(df_data_2$YearMonth, "-01"))

# Create the plots
p1 <- ggplot(df_data_2, aes(x = YearMonth, y = MA_1)) +
  geom_line() +
  ggtitle("1-Month Moving Average of DMA A")

p5 <- ggplot(df_data_2, aes(x = YearMonth, y = MA_5)) +
  geom_line(color = "blue") +
  ggtitle("5-Month Moving Average of DMA A")

p10 <- ggplot(df_data_2, aes(x = YearMonth, y = MA_10)) +
  geom_line(color = "red") +
  ggtitle("10-Month Moving Average of DMA A")

p24 <- ggplot(df_data_2, aes(x = YearMonth, y = MA_24)) +
  geom_line(color = "green") +
  ggtitle("24-Month Moving Average of DMA A")

# Display the plots
library(gridExtra)
grid.arrange(p1, p5, p10, p24, ncol = 2)

```




## Punto 1.3
Asimimso, se el pide que busque variables que tengan una relación con el consumo de cada una de las áreas mostradas en la tabla 1.



- Correlación con la temperatura: Las zonas DMA presentan una correlación positiva entre 0.51 y 0.58 con la temperatura del aire, lo que indica que el consumo de agua aumenta con la temperatura, especialmente en épocas calurosas.

- Influencia de la velocidad del viento y la lluvia: La velocidad del viento tiene poco impacto en el consumo de agua en las zonas DMA, con correlaciones cercanas a cero. Sin embargo, la lluvia muestra una leve correlación positiva en DMA C, sugiriendo un aumento en el consumo de agua durante días lluviosos, posiblemente por la recolección de agua pluvial.


## Bono
El dataset inclute posible outliers. Identifique, elimine y compare los resultados obtenidos.


### Metrica para calcular el K utilizando Kmeans

Para esto utilizamos una gráfica Within-Cluster Sum of Squares WSS y probamos para 20 valores de K, en este caso tenemos la siguiente gráfica


```{r}

# Extract only numeric columns from the dataframe
numeric_data <- df_data_1[, sapply(df_data_1, is.numeric)]

# Initialize WSS vector
wss <- numeric(k.max)

# Calculate WSS for each k
for (k in 1:k.max) {
  wss[k] <- sum(kmeans(numeric_data, centers = k, nstart = 20)$withinss)
}

## Warning: did not converge in 10 iterations

library(ggplot2)

k_values <- 1:k.max  # the range of k you tested
ggplot(data.frame(k = k_values, WSS = wss), aes(x = k, y = WSS)) +
  geom_point() +
  geom_line() +
  labs(title = "Elbow Method for Choosing k",
       x = "Number of clusters k",
       y = "Total within-cluster sum of squares") +
  theme_minimal()

```

Observamos que tenemos un valor bajo de WSS, que cuantifica la compacidad de los clusters entre sí. En este caso, solo probamos con 10 diferentes clusters.Ahora vamos a utilizar estos clusters y verlos utilizando PCA.


### K elegido

```{r}
numeric_df <- df_data_1 %>%
  select_if(is.numeric)

# Perform PCA on numeric data
pca_result <- prcomp(numeric_df, scale. = TRUE)

pca_df <- as.data.frame(pca_result$x)  # Use principal components as dataframe


k <- 10
km_res <- kmeans(pca_df[, c("PC1", "PC2")], centers = k, nstart = 25)

# Add the cluster assignments to your dataframe
pca_df$cluster <- as.factor(km_res$cluster)
library(ggplot2)

# Plot with ggplot2
library(ggplot2)

# Assuming pca_df already has 'cluster' column from k-means clustering
ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.9) + # Use alpha to adjust point opacity, improving plot readability
  scale_color_brewer(palette = "Set3") + # Use a distinct color palette for clarity
  labs(title = "K-means Clustering with k=10 on PCA Components 1 and 2",
       x = "Principal Component 1 (PC1)",
       y = "Principal Component 2 (PC2)",
       color = "Cluster") + # Correct way to set the legend title
  theme_minimal() # Use a minimal theme for a cleaner look

print(summary(pca_result)$importance[2, 1:2])

```
En nuestro conjunto de datos, los dos primeros componentes principales explican el 60.401% de la varianza, lo cual es excelente. Esto significa que, utilizando solo dos variables, logramos capturar una gran parte de la información contenida en nuestros datos. Para visualizar cómo estos componentes se relacionan con la estructura de los datos, elaboramos un gráfico de dispersión de los PCA, donde observamos que nuestros 10 clusters se ajustan adecuadamente a los datos.

```{r}
# Calculate centroids from the kmeans result
centroids <- aggregate(cbind(PC1, PC2) ~ cluster, data = pca_df, FUN = mean)

# Join centroids back to the original data to compute distances
pca_df_with_centroids <- merge(pca_df, centroids, by = "cluster", suffixes = c("", "_centroid"))

# Calculate Euclidean distance from each point to its cluster centroid
pca_df_with_centroids$distance <- sqrt((pca_df_with_centroids$PC1 - pca_df_with_centroids$PC1_centroid)^2 + 
                                       (pca_df_with_centroids$PC2 - pca_df_with_centroids$PC2_centroid)^2)

# Define outlier criterion, e.g., distances greater than 2 standard deviations from the mean distance for each cluster
pca_df_with_centroids <- pca_df_with_centroids %>%
  group_by(cluster) %>%
  mutate(mean_distance = mean(distance), sd_distance = sd(distance)) %>%
  ungroup() %>%
  mutate(outlier = ifelse(distance > mean_distance + 1.5*sd_distance, TRUE, FALSE))

# Filter out outliers
df_no_outliers <- filter(pca_df_with_centroids, outlier == FALSE)


```

Tras identificar nuestros clusters, procedemos a calcular los centroides de cada uno para determinar qué valores se desvían significativamente dentro de cada grupo. Utilizamos un criterio de 1.5 desviaciones estándar alrededor del centroide de cada cluster para identificar y excluir los outliers de nuestro análisis. Esto nos permite depurar nuestro conjunto de datos, eliminando elementos atípicos que podrían distorsionar los resultados. Como resultado, generamos un nuevo dataframe, denominado df_no_outliers.

### Resultdos Sin Outliers


Ahora vamos a hacer el análisis sin outliers, primero veamos la cantidad de Outliers que quitamos de nuestros datos.


```{r}
# Store the dimensions of each dataframe
dimensions_df <- dim(df_data_1)
dimensions_no_outliers <- dim(df_no_outliers)

# Calculate the difference in dimensions
difference <- dimensions_df - dimensions_no_outliers

# Print dimensions and difference in one statement
cat("Dimensions of df_data_1:", paste(dimensions_df, collapse = "x"), "\n",
    "Dimensions of df_no_outliers:", paste(dimensions_no_outliers, collapse = "x"), "\n",
    "Difference (outliers removed):", paste(difference, collapse = "x"), "\n")

```
Aquí vemos que hemos eliminado 613 valores atípicos en nuestros datos.


```{r}


data_2 <- df_data_1 %>%
  select('Date',`DMA A`, `DMA B`, `DMA C`, `DMA D`, `DMA E`, `DMA F`, `DMA G`, `DMA H`, `DMA I`, `DMA J`,
         `Rainfall depth (mm)`, `Air temperature (°C)`, `Air humidity (%)`, `Windspeed (km/h)`)

row_indices <- as.numeric(rownames(df_no_outliers))

# Now you can use these indices to select the same rows from the original dataframe
df_no_outliers <- data_2[row_indices, ]

```



```{r}
df_no_outliers$Date <- as.POSIXct(df_no_outliers$Date, format="%d/%m/%Y %H:%M")
df_no_outliers$Hour <- hour(df_no_outliers$Date)

hourly_averages <- df_no_outliers %>%
  group_by(Hour) %>%
  summarise(Average_DMA_A = mean(`DMA A`, na.rm = TRUE))

# Creamos el gráfico con ggplot2
ggplot(hourly_averages, aes(x = Hour, y = Average_DMA_A)) +
  geom_line() +  # Línea que conecta los puntos medios
  geom_point() +  # Puntos que representan los promedios reales por hora
  labs(title = "Media Móvil por Hora para DMA A", x = "Hora del Día", y = "Media de DMA A") +
  scale_x_continuous(breaks = 0:23) +  # Aseguramos que el eje x tenga las 24 horas
  theme_minimal()  # Tema minimalista para el gráfico
```
Observamos que en la gráfica de horas, la visualización es un poco más clara, pero en general, no se observan diferencias muy notables con los datos sin outliers. La media no parece verse afectada significativamente en este caso.
```{r}

df_data_2 <- df_no_outliers %>%
  mutate(YearMonth = format(Date, "%Y-%m")) %>%
  group_by(YearMonth) %>%
  summarise(Monthly_Avg_DMA_A = mean(`DMA A`, na.rm = TRUE)) %>%
  ungroup()

# Calculamos las medias móviles para cada uno de los periodos solicitados
# Change the names of the moving averages columns
df_data_2$MA_1 <- rollapply(df_data_2$Monthly_Avg_DMA_A, width = 1, FUN = mean, fill = NA, align = 'right')
df_data_2$MA_5 <- rollapply(df_data_2$Monthly_Avg_DMA_A, width = 2, FUN = mean, fill = NA, align = 'right')
df_data_2$MA_10 <- rollapply(df_data_2$Monthly_Avg_DMA_A, width = 5, FUN = mean, fill = NA, align = 'right')
df_data_2$MA_24 <- rollapply(df_data_2$Monthly_Avg_DMA_A, width = 10, FUN = mean, fill = NA, align = 'right')

# Convert YearMonth back to a date object for visualization
df_data_2$YearMonth <- as.Date(paste0(df_data_2$YearMonth, "-01"))

# Create the plots
p1 <- ggplot(df_data_2, aes(x = YearMonth, y = MA_1)) +
  geom_line() +
  ggtitle("1-Month Moving Average of DMA A")

p5 <- ggplot(df_data_2, aes(x = YearMonth, y = MA_5)) +
  geom_line(color = "blue") +
  ggtitle("5-Month Moving Average of DMA A")

p10 <- ggplot(df_data_2, aes(x = YearMonth, y = MA_10)) +
  geom_line(color = "red") +
  ggtitle("10-Month Moving Average of DMA A")

p24 <- ggplot(df_data_2, aes(x = YearMonth, y = MA_24)) +
  geom_line(color = "green") +
  ggtitle("24-Month Moving Average of DMA A")

# Display the plots
library(gridExtra)
grid.arrange(p1, p5, p10, p24, ncol = 2)

```

Y también para los intervalos de 1,5, 10 y 24 meses no se ve mucha la diferencia, cuando quitamos los outliers para la gráfica del DMA A


