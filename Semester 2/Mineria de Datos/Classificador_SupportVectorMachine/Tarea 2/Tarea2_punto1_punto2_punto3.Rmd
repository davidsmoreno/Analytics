---
title: "Tarea 2 Mineria de Datos"
author: "David Romero, Luisa De La Hortua y David Moreno"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: cosmo
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PUNTO 1

Se sabe que las m√°quinas de soporte vectorial (SVM) se pueden ajustar con un kernel no lineal para resolver un problema de clasificaci√≥n con una decisi√≥n de frontera no lineal. Se quiere revisar qu√© pasar√≠a si se aborda una regresi√≥n log√≠stica usando una transformaci√≥n no lineal de las caracter√≠sticas. Para ello contemple lo siguiente:

## **Punto a.**

Genere un conjunto de datos con ùëõ=500 observaciones y ùëù=2 categor√≠as con una frontera de decisi√≥n cuadr√°tica usando lo planteado

```{r cars}
# Cargar la librer√≠a necesaria para graficar
library(ggplot2)

# Generar datos
set.seed(43) # Para reproducibilidad
n <- 500
X1 <- runif(n, -0.5, 0.5)
X2 <- runif(n, -0.5, 0.5)
y <- ifelse(X1^2 - X2^2 > 0, 1, 0)

datos <- data.frame(X1, X2, y)
datos$y <- as.factor(datos$y)
head(datos)
```

## **Punto b.**

Grafique las observaciones incluyendo colores que diferencien ambas categor√≠as. Utilice como eje x el valor de ùëã1 y en el eje y el valor de ùëã2. ¬øQu√© puede decir sobre la regi√≥n de clasificaci√≥n?

```{r cars 2}
# Visualizar los datos
ggplot(datos, aes(x=X1, y=X2, color=factor(y))) + 
  geom_point() + 
  ggtitle("Distribuci√≥n de datos con frontera de decisi√≥n cuadr√°tica") + 
  scale_color_manual(values=c('red', 'blue'), labels=c('Categor√≠a 0', 'Categor√≠a 1')) + 
  labs(color = "Categor√≠a") +
  theme_minimal()

```

La frontera de decisi√≥n que separa las dos categor√≠as en el conjunto de datos es una hip√©rbola, donde los puntos que cumplen con la condici√≥n dada  caen en una regi√≥n que incluye partes del primer y tercer cuadrante, representando una categor√≠a, mientras que los puntos que no cumplen con esta condici√≥n caen en la otra categor√≠a, abarcando regiones del segundo y cuarto cuadrante. Esta divisi√≥n no lineal indica que la relaci√≥n entre las dos variables no puede ser capturada adecuadamente por una l√≠nea recta.

## **Punto c.**

Ajuste una regresi√≥n log√≠stica usando los datos generados y ùëã1 y ùëã2 como predictores. ¬øQu√© puede decir sobre la capacidad de predicci√≥n?

```{r cars 3}
library(caTools)

# Ajustar el modelo de regresi√≥n log√≠stica
modelo <- glm(y ~ X1 + X2, family = binomial, data = datos)

# Predicciones y evaluaci√≥n del modelo
predicciones <- predict(modelo, newdata = datos, type = "response")
clasificacion <- ifelse(predicciones > 0.5, 1, 0)
tabla <- table(datos$y, clasificacion)
accuracy <- sum(diag(tabla)) / sum(tabla)

summary(modelo)

# Resultado
cat("La precisi√≥n del modelo es:", accuracy)
```

Con una precisi√≥n del modelo de aproximadamente 48.7%, la capacidad de predicci√≥n de la regresi√≥n log√≠stica con X1 y X2  como predictores es bastante baja y no es mejor que la clasificaci√≥n aleatoria. El modelo lineal de la regresi√≥n log√≠stica no es adecuado para capturar la relaci√≥n no lineal que se observo anteriormente entre las variables predictoras y la variable respuesta en estos datos. Adem√°s se observa que las variables no son significativas al 5% ya que sus p valores son muy superiores a un 0.05.

## **Punto d.**

Aplique este modelo a los datos de entrenamiento para obtener una clase predicha para cada observaci√≥n. Grafique las observaciones usando como colores las clases predichas. ¬øQu√© forma tiene esta frontera? Comente sus resultados.

```{r cars 48}
# Predecir las clases para el conjunto de entrenamiento
datos$predicha <- predict(modelo, newdata = datos, type = "response") > 0.5

accuracy_1 <- mean(datos$predicha == as.numeric(as.character(datos$y)))
print(paste("La precisi√≥n del modelo logistico  es:", accuracy_1))
# Graficar las observaciones con las clases predichas
ggplot(datos, aes(x = X1, y = X2, color = factor(predicha))) +
  geom_point() +
  scale_color_manual(values = c('red', 'blue'), labels = c('Predicha 0', 'Predicha 1')) +
  labs(color = "Clase Predicha") +
  ggtitle("Observaciones con Clases Predichas") +
  theme_minimal()

```

La gr√°fica muestra claramente que una frontera de decisi√≥n lineal no es adecuada para la disposici√≥n actual de los datos, que parece ajustarse m√°s a una curva hip√©rbola que a una l√≠nea recta. El resultado muestra la limitaci√≥n de usar un modelo de regresi√≥n log√≠stica simple cuando los datos exhiben una relaci√≥n no lineal m√°s compleja. 

## **Punto e.**

Ahora ajuste un modelo de regresi√≥n log√≠stica a los datos usando funciones no lineales de ùëã1 y ùëã2 como predictores (por ejemplo, ùëã12,ùëã1‚àóùëã2,ùëã22, entre otros) y relacione los resultados del modelo.

```{r cars 4}
# Cargar la librer√≠a necesaria
library(glm2)

# A√±adir nuevas variables al conjunto de datos
datos$X1_sq <- datos$X1^2
datos$X1_X2 <- datos$X1 * datos$X2
datos$X2_sq <- datos$X2^2

# Ajustar el modelo de regresi√≥n log√≠stica con las nuevas variables
modelo_nl <- glm(y ~ X1 + X2 + X1_sq + X1_X2 + X2_sq, family = binomial, data = datos)

# Resumen del modelo para ver los resultados
summary(modelo_nl)


```

La regresi√≥n logistica muestra que cada uno de los coeficientes es altamente significativo, como lo indican los valores p extremadamente bajos

## **Punto f.**

Aplique el modelo a los datos de entrenamiento para obtener datos predichos para cada observaci√≥n. Grafique las observaciones usando los colores predichos. Compare la frontera de decisi√≥n con la obtenida anteriormente.

```{r cars 45}

# Predecir las clases para el conjunto de entrenamiento utilizando el nuevo modelo
datos$predicha_nl <- predict(modelo_nl, newdata = datos, type = "response") > 0.5

# Calcular la precisi√≥n del nuevo modelo en el conjunto de entrenamiento
accuracy_nl <- mean(datos$predicha_nl == as.numeric(as.character(datos$y)))
print(paste("La precisi√≥n del modelo con predictores no lineales en el conjunto de entrenamiento es:", accuracy_nl))

# Opcional: Graficar las observaciones con las clases predichas por el nuevo modelo
ggplot(datos, aes(x = X1, y = X2, color = factor(predicha_nl))) +
  geom_point() +
  scale_color_manual(values = c('red', 'blue'), labels = c('Predicha 0', 'Predicha 1')) +
  labs(color = "Clase Predicha (Modelo No Lineal)") +
  ggtitle("Observaciones con Clases Predichas por Modelo No Lineal ") +
  theme_minimal()


```

La nueva representaci√≥n gr√°fica muestra una mejora en la clasificaci√≥n de los datos, con un aumento  en la precisi√≥n del modelo al pasar de un 54.6% a un 96.6%. Esta mejora evidencia la efectividad de incorporar t√©rminos no lineales en el modelo. La frontera de decisi√≥n, a diferencia de la vista anteriormente, ahora obtiene la forma de hip√©rbolica que describe los datos.

## **Punto g.**

Ajuste un clasificador de vectores de soporte a los datos de ùëã1 y ùëã2 como predictores. Obtenga una predicci√≥n para cada observaci√≥n y su correspondiente figura usando los colores predichos.

```{r cars 66}
library(e1071)
library(ggplot2)


# Ajustar el clasificador SVM
modelo_svm <- svm(formula = y ~ X1 + X2, data = datos, kernel = "linear",
                  cost = 10, scale = TRUE)

# Obtener predicciones para cada observaci√≥n
predicciones_svm <- predict(modelo_svm, datos[, c("X1", "X2")]) 

# A√±adir las predicciones al data.frame
datos$predicciones_svm <- predicciones_svm

# Resumen del SVM lineal
summary(modelo_svm)


# Calcular la precisi√≥n del modelo SVM
accuracy_svm <- mean(datos$predicciones_svm == as.numeric(as.character(datos$y)))
print(paste("La precisi√≥n del clasificador SVM es:", accuracy_svm))

# Graficar las observaciones con las clases predichas por el modelo SVM
ggplot(datos, aes(x = X1, y = X2, color = factor(predicciones_svm))) +
  geom_point() +
  scale_color_manual(values = c('red', 'blue'), labels = c('Predicha 0', 'Predicha 1')) +
  labs(color = "Clase Predicha (Modelo SVM)") +
  ggtitle("Observaciones con Clases Predichas por Modelo lineal SVM") +
  theme_minimal()


plot(modelo_svm, datos, X1 ~ X2)
```
La gr√°fica muestra las observaciones con las clases predichas por un modelo de SVM lineal. Se puede observar que todas las observaciones est√°n clasificadas en una √∫nica categor√≠a, lo cual indica que el modelo no est√° discriminando adecuadamente entre las dos clases. La precisi√≥n del modelo, que es ligeramente superior a la aleatoriedad con un valor de 0.51, refuerza esta idea al respecto 

## **Punto h.**

Ajuste ahora un SVM usando un kernel no lineal de tipo polinomial y radial. Obtenga una clase para cada observaci√≥n. Grafique la frontera de decisi√≥n con base en las clases predichas

### Radial

Primero obtendremos los mejores hiperparametros para el radial

```{r cars 667}

set.seed(1)

# Definicion de grilla para buscar mejores parametros
svm_cv <- tune("svm", y ~ X1 + X2, data = datos, kernel = 'radial',
               ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 20),
                             gamma = c(0.5, 1, 2, 3, 4, 5, 10)))
# Grafica
ggplot(data = svm_cv$performances, aes(x = cost, y = error, color = as.factor(gamma)))+
  geom_line() +
  geom_point() +
  labs(title = "Error de clasificaci√≥n vs hiperpar√°metros C y gamma", color = "gamma") +
  theme_bw() +
  theme(legend.position = "bottom")
# Mejores parametros
svm_cv$best.parameters

```

Con los mejores hiperparametros ajustamos el svm radial

```{r cars 67}
# Ajustar el clasificador SVM con un kernel radial
modelo_svm_radial <- svm(formula = y ~ X1 + X2, data = datos, kernel = "radial", cost = 20, gamma = 2, scale = TRUE)

# Resumen del SVM radial
summary(modelo_svm_radial)

# Obtener predicciones para cada observaci√≥n
predicciones_svm_radial <- predict(modelo_svm_radial, datos)

# A√±adir las predicciones al data.frame
datos$predicciones_svm_radial <- predicciones_svm_radial

# Calcular la precisi√≥n del modelo SVM radial
accuracy_svm_radial <- mean(datos$predicciones_svm_radial == as.numeric(as.character(datos$y)))
print(paste("La precisi√≥n del clasificador SVM radial es:", accuracy_svm_radial))

# Graficar las observaciones con las clases predichas por el modelo SVM radial
ggplot(datos, aes(x = X1, y = X2, color = factor(predicciones_svm_radial))) +
  geom_point() +
  scale_color_manual(values = c('red', 'blue'), labels = c('Predicha 0', 'Predicha 1')) +
  labs(color = "Clase Predicha (Modelo SVM Radial)") +
  ggtitle("Observaciones con Clases Predichas por Modelo SVM Radial") +
  theme_minimal()

plot(modelo_svm_radial, datos, X1 ~ X2)

```

El SVM con kernel radial tiene una alta precisi√≥n con un 98.4%, muestra un ajuste excelente a los datos La gr√°fica de las clases predichas muestra una clara y definida separaci√≥n entre las dos categor√≠as, evidenciando la capacidad del kernel radial para tratar la no linealidad del conjunto de datos. 

### Polinomial

Primero obtendremos los mejores hiperparametros para el polinomial

```{r cars 18, warning=FALSE}

set.seed(1)

svm_cv <- tune(svm, y ~ X1 + X2, data = datos, kernel = 'polynomial',
               ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 20),
                             gamma = c(0.5, 1, 2, 3, 4, 5, 10),
                             degree = c(2)), 
               tunecontrol = tune.control(sampling = "cross", cross = 5))


# Grafica
ggplot(data = svm_cv$performances, aes(x = cost, y = error, color = as.factor(gamma)))+
  geom_line() +
  geom_point() +
  labs(title = "Error de clasificaci√≥n vs hiperpar√°metros C y gamma", color = "gamma") +
  theme_bw() +
  theme(legend.position = "bottom")

# Mejores parametros
svm_cv$best.parameters

```

Con los mejores hiperparametros ajustamos el svm polinomial

```{r cars 19}
# Ajustar el clasificador SVM con un kernel polinomial
modelo_svm_poly <- svm(y ~ X1 + X2, data = datos, kernel = "polynomial",
                       cost = 10, gamma = 1, degree = 2, scale = TRUE)


# Resumen del SVM radial
summary(modelo_svm_poly)

# Obtener predicciones para cada observaci√≥n
predicciones_svm_poly <- predict(modelo_svm_poly, datos)

# A√±adir las predicciones al data.frame
datos$predicciones_svm_poly <- predicciones_svm_poly

# Calcular la precisi√≥n del modelo SVM radial
accuracy_svm_poly <- mean(datos$predicciones_svm_poly == as.numeric(as.character(datos$y)))
print(paste("La precisi√≥n del clasificador SVM radial es:", accuracy_svm_poly))

# Graficar las observaciones con las clases predichas por el modelo SVM radial
ggplot(datos, aes(x = X1, y = X2, color = factor(predicciones_svm_poly))) +
  geom_point() +
  scale_color_manual(values = c('red', 'blue'), labels = c('Predicha 0', 'Predicha 1')) +
  labs(color = "Clase Predicha (Modelo SVM Polinomial)") +
  ggtitle("Observaciones con Clases Predichas por Modelo SVM Polinomial") +
  theme_minimal()

plot(modelo_svm_poly, datos, X1 ~ X2)

```

El clasificador SVM con kernel polinomial alcanza una precisi√≥n del 98.2%, obteniendo un excelente rendimiento. Las gr√°ficas de las clases predichas  muestran una separaci√≥n  marcada como la observada con el kernel radial (el kernel polinomico obtiene mejores resultados),  aunque el kernel radial a√±ade cierta capacidad para capturar la no linealidad de los datos, la mejor opci√≥n es usar el kernel polinomico. 


## **Punto i.**

Analice los resultados obtenidos en cada inciso. ¬øQu√© conclusiones obtiene frente a estas herramientas y este tipo de problemas de clasificaci√≥n?


En cada inciso se realizo un an√°lisis de los resultados, sin embargo, se puede concluir que:

La capacidad de un modelo para manejar la complejidad de los datos es fundamental para su rendimiento en problemas de clasificaci√≥n. En este caso, donde la verdadera frontera de decisi√≥n es cuadr√°tica, los modelos lineales (regresi√≥n log√≠stica y SVM lineal) tienen un desempe√±o cercano a una elecci√≥n aleatoria, con precisiones de 54.6% y 51.2% respectivamente, lo que es de esperar dado que no pueden capturar la relaci√≥n no lineal intr√≠nseca en los datos.

Por otro lado, al incorporar t√©rminos polin√≥micos en la regresi√≥n log√≠stica, la precisi√≥n aument√≥ significativamente a 96.6%, indicando que el modelo pudo adaptarse mejor a la estructura no lineal de los datos. Similarmente, el SVM con kernel radial tambi√©n mostr√≥ una alta precisi√≥n de 98.4%.

El SVM con kernel polinomial tuvo un rendimiento muy alto  con una precisi√≥n del 98.2%, captura perfectamente la relacion cuadratica con la que fue creado el dataset.

Para este tipo de problemas de clasificaci√≥n no lineales, los modelos que pueden incorporar la no linealidad ya sea mediante transformaciones de los predictores o a trav√©s de kernels como el radial o polinomico, tienden a tener un rendimiento superior. 

# PUNTO 2

```{r Librerias, message=FALSE, warning=FALSE}
library(ISLR)
library(tidyr) 
library(splines)
library(gam)
library(corrplot)
library(MASS)
library(caret)
```

Para este punto considere el conjunto de datos de Auto disponible en Bloque Ne√≥n.

```{r Auto}
data("Auto")
str(Auto)
attach(Auto)
# Carga el paquete openxlsx
library(openxlsx)



# Escribe el dataframe en un archivo xlsx
write.xlsx(Auto, "Auto.xlsx")
```

## **Punto a.** 

Resuma Cree una variable binaria que tome valores de 1 para los carros que tengan un kilometraje por gal√≥n (mpg) por encima de la mediana y 0 en caso contrario.

```{r Punto a, warning=FALSE}

# Calcular la mediana del vector de mpg
mediana_mpg <- median(Auto$mpg)
print(mediana_mpg)
# Crear la variable binaria dentro del dataframe
Auto$mpg_binaria <- ifelse(Auto$mpg > mediana_mpg, 1, 0)

Auto <- Auto[, -which(names(Auto) == 'mpg')]
# Establecer una semilla para reproducibilidad
set.seed(1)

# Crear la partici√≥n de los datos
indices <- createDataPartition(y = Auto$mpg_binaria, p = 0.7, list = FALSE)

# Crear conjunto de entrenamiento
Auto_entrenamiento <- Auto[indices, ]

# Crear conjunto de prueba
Auto_prueba <- Auto[-indices, ]

Auto_entrenamiento$mpg_binaria <- as.factor(Auto_entrenamiento$mpg_binaria)
Auto_prueba$mpg_binaria <- as.factor(Auto_prueba$mpg_binaria)

```

##**Punto b.** 

Ajuste un clasificador de vectores de soporte a los datos con varios valores de costo. Reporte los errores por cros-validaci√≥n obtenidos con los diferentes par√°metros. Comente los resultados.

```{r Punto b, warning=FALSE}
library(e1071)
set.seed(1)
tune.out <- tune(svm, mpg_binaria ~ ., data = Auto_entrenamiento, kernel = "linear", 
    ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
# Ajustar el modelo SVM con el mejor par√°metro obtenido
bestmod <- svm(mpg_binaria ~ ., data = Auto_entrenamiento, kernel = "linear", cost =0.1)
summary(bestmod)
ypred <- predict(bestmod,Auto_prueba )

table(predict = ypred, truth = Auto_prueba$mpg_binaria)
```

En este caso se evidencia como el mejor modelo es el que tiene un error de validaci√≥n cruzada de 0.07314815 con un costo de 0.1. Este modelo nos calsifica correctamente 104 de los 116 registros evaluados. 

## **Punto c.** 

Repita el inciso anterior usando un SVM con un kernel radial y polinomial con diferentes valores de gamma, degree y cost. Comente los resultados obtenidos

### Kernel Radial

```{r Punto c Radial, warning=FALSE}
# Definir cuadr√≠culas de par√°metros
gamma_values <- c(0.001,0.01, 0.1, 1, 5, 10, 100)
degree_values <- c(0.001,2, 3, 4, 5)
cost_values <- c(0.001,0.01, 0.1, 1, 5, 10, 100)

# Ajustar el modelo SVM con kernel radial
tune_radial <- tune(svm, 
                    mpg_binaria ~ ., 
                    data = Auto_entrenamiento, 
                    kernel = "radial",
                    ranges = list(gamma = gamma_values, 
                                  cost = cost_values,
                                  degree = degree_values))

summary(tune_radial)

# Ajustar el modelo SVM con el mejor par√°metro obtenido
bestmod_rad <- svm(mpg_binaria ~ ., data = Auto_entrenamiento, kernel = "radial", cost = tune_radial$best.parameters$cost, gamma=tune_radial$best.parameters$gamma, degree=tune_radial$best.parameters$degree)
summary(bestmod_rad)
ypred <- predict(bestmod_rad,Auto_prueba )

table(predict = ypred, truth = Auto_prueba$mpg_binaria)
```

Para el Kernel radial se encontr√≥ que el mejor modelo que se ajustaba es el que tiene los parametros costo:1, gamma: 0.01 y degree:0.001. El error del validaci√≥n cruzada en este caso es de 0.07248677.

### Kernel Polinomial 

```{r Punto c Polinomial, warning=FALSE}
# Ajustar el modelo SVM con kernel polinomial
tune_polynomial <- tune(svm, 
                        mpg_binaria ~ ., 
                        data = Auto_entrenamiento, 
                        kernel = "polynomial",
                        ranges = list(degree = degree_values, cost = cost_values, gamma=gamma_values))

summary(tune_polynomial)

# Ajustar el modelo SVM con el mejor par√°metro obtenido
bestmod_pol <- svm(mpg_binaria ~ ., data = Auto_entrenamiento, kernel = "polynomial", cost = tune_polynomial$best.parameters$cost, gamma=tune_polynomial$best.parameters$gamma, degree=tune_polynomial$best.parameters$degree)
summary(bestmod_pol)
ypred <- predict(bestmod_pol,Auto_prueba )

table(predict = ypred, truth = Auto_prueba$mpg_binaria)
```

Para el Kernel polinomial se encontr√≥ que el mejor modelo que se ajustaba es el que tiene los parametros costo:100, gamma:0.1 y degree:3. El error del validaci√≥n cruzada en este caso es de 0.08703704.

Por lo tanto, el modelo que nos deja un error de validaci√≥n cruzada m√°s bajo es el trabaja con el kernel radial.

## **Punto d.** 

Haga algunas figuras para soportar sus conclusiones en los incisos b) y c) usando la funci√≥n plot y sabiendo que se pueden graficar por parejas de variables usando plot(svmfit, dat, x1~x4) reemplazando los nombres de x1 y x4 correspondientes.

```{r Punto d Lineal, warning=FALSE}
# Obtener el nombre de las variables predictoras
variables_predictoras <- names(Auto_entrenamiento)[!names(Auto_entrenamiento) %in% c("mpg_binaria", "name")]

## Graficar la variable de respuesta contra cada variable predictora
for (variable in variables_predictoras) {
  for (variable_2 in variables_predictoras){
    if (variable_2 == variable){
      next
    }
    else {
      plot(bestmod, Auto_entrenamiento, formula = as.formula(paste(variable,"~", variable_2)))
    }
  }
}
```


```{r Punto d Radial, warning=FALSE}
## Graficar la variable de respuesta contra cada variable predictora
for (variable in variables_predictoras) {
  for (variable_2 in variables_predictoras){
    if (variable_2 == variable){
      next
    }
    else {
      plot(bestmod_rad, Auto_entrenamiento, formula = as.formula(paste(variable,"~", variable_2)))
    }
  }
}
```

```{r Punto d Polinomial, warning=FALSE}
for (variable in variables_predictoras) {
  for (variable_2 in variables_predictoras){
    if (variable_2 == variable){
      next
    }
    else {
      plot(bestmod_pol, Auto_entrenamiento, formula = as.formula(paste(variable,"~", variable_2)))
    }
  }
}
```

## **Punto e.** 

Analice los resultados obtenidos.

Considerando las gr√°ficas analizadas anteriormente, resulta desafiante tomar decisiones precisas debido a la limitaci√≥n que presentan la mayor√≠a de las visualizaciones al mostrar comportamientos complejos en solo dos variables. Sin embargo, destaca que el modelo SVM radial se ajusta de manera m√°s adecuada a este tipo de problemas. Este modelo exhibe un menor error en la validaci√≥n cruzada y clasifica correctamente un mayor n√∫mero de registros en la respuesta deseada.

# PUNTO 3



Primero vamos a cargar los datos OJ, y mostramos los primeros valores de nuestro dataset.

```{r, echo=FALSE}
load("OJ.Rdata")

head(OJ)

library(e1071)
library(caret)
library(ggplot2)#Importar las librerias

```

Realizamos un resumen estad√≠stico para obtener una visi√≥n general de las caracter√≠sticas descriptivas de nuestros datos.

## Exploraci√≥n de las variables

```{r}
summary(OJ)
```


Veamos las columnas de los datos, tenemos la variable "Purchase", que es binaria y distingue entre dos categor√≠as, para cada categoria hay 653 registros de "CH" y 417 de "MM". La variable *WeekofPurchase* nos dice en qu√© semana se realiz√≥ cada compra con semanas que van desde la 227 a la 278. Para la columna *StoreId*, tenemos 7 categor√≠as unicas, qu√© muy posiblemente sean los Ids de los Stores donde se hizo esta compra.

En cuanto a los precios, *PriceCH* y *PriceMM* nos indican los precios de cada categor√≠a, variando de $1.69 a $2.09 para "CH" y de $1.69 a $2.29 para "MM", lo cual nos sugiere que "MM" tiende a tener precios ligeramente m√°s altos. Los descuentos aplicados a cada marca, indicados por *DiscCH* y *DiscMM*, var√≠an de $0.00 a $0.50 y de $0.00 a $0.80 respectivamente, mostrando una mayor rango en "MM".

*SpecialCH* y *SpecialMM* parecen ser indicadores binarios (1 o 0) que nos dicen si hubo alguna promoci√≥n especial en el momento de la compra. La lealtad hacia la marca "CH", medida por "LoyalCH", se representa en una escala de 0 a 1, donde 1 es la m√°xima lealtad.

Las variables de precios de venta *SalesPriceMM* y *SalesPriceCH*, la variable *PriceDiff* que tiene que ver con alguna diferencia entre precios. La variable *Store7* que es una variable binaria que nos dice si la compra se realiz√≥ en la tienda n√∫mero 7 o no. y *Store* que nos indica la tienda.

 
## Divisi√≥n: Test y Train (**Punto a.**) 


```{r}
set.seed(123) # Para reproducibilidad
indices <- sample(1:nrow(OJ), 800)#Elegimos 800 muestras

#Creamos los datos de prueba y test(Utilizando nuestra muestra anterior).
train_data <- OJ[indices, ]
test_data <- OJ[-indices, ]

```


## Ajuste SVV(**Punto b.**)


```{r}

# Ajuste del modelo SVC(Purchase contra las dem√°s, y el kernel linear con el costo de 0.01)
svc_model <- svm(Purchase ~ ., data = train_data, kernel = "linear", cost = 0.01)

# Resumen del modelo
summary(svc_model)

```

Aqu√≠ entrenamos el modelo utilizando los datos de entrenamiento y revisamos el resumen de este modelo. Se configura el modelo para que sea lineal, con un costo de 0.01, y observamos que tiene un total de 442 vectores de soporte, distribuidos como 220 para la categor√≠a 1 y 222 para la categor√≠a 2. Estas categor√≠as corresponden a las etiquetas que identificamos previamente en el an√°lisis exploratorio de los datos como *CH* y *MM*.


### Evaluaci√≥n Train (**Punto c.**)

```{r,}
# Realizamos las predicciones en el conjunto de entrenamiento
pred_train <- predict(svc_model, train_data)

# Agregamos la matriz de confusi√≥n
confusion_matrix_train <- table(pred_train, train_data$Purchase)
print("Matriz de Confusi√≥n en los datos de entrenamiento:")
print(confusion_matrix_train)

# Calculamos la precisi√≥n
accuracy_train <- sum(pred_train == train_data$Purchase) / nrow(train_data)
print(paste("Precisi√≥n en los datos de entrenamiento:", accuracy_train))

```


La matriz de confusi√≥n muestra correctamente 426 instancias como CH y 242 instancias como MM. No obstante, cometi√≥ errores clasificando 61 instancias de MM como CH y 72 instancias de CH como MM. Al calcular el *Accuracy* obtenemos un valor de , (145 + 77) / (145 + 27 + 21 + 77) = 222 / 270 ‚âà 0.822%. Lo cual significa que el modelo predijo correctamente el 83.5% de los datos del conjunto de entrenamiento. A pesar de este alto nivel de precisi√≥n en los datos de entrenamiento, hay que tener cuidad. Un desempe√±o alto en el entrenamiento pero no igualmente alto en los datos de prueba puede ser se√±al de un sobreajuste.


### Evaluaci√≥n Test (**Punto c.**)

```{r,}
# Realizamos las predicciones en el conjunto de prueba
pred_test <- predict(svc_model, test_data)

# Agregamos la matriz de confusi√≥n
confusion_matrix_test <- table(pred_test, test_data$Purchase)
print("Matriz de Confusi√≥n en los datos de prueba:")
print(confusion_matrix_test)

# Calculamos la precisi√≥n
accuracy_test <- sum(pred_test == test_data$Purchase) / nrow(test_data)
print(paste("Precisi√≥n en los datos de prueba:", accuracy_test))
```

Para la clase CH, el modelo predijo correctamente 145 instancias, pero clasific√≥ err√≥neamente 27 instancias como pertenecientes a la clase MM. Por el contrario, para la clase MM, el modelo predijo correctamente 77 instancias mientras que predijo incorrectamente 21 instancias como clase CH. Alcanzo un accurracy de 0.82 lo cual est√° bastante bien para solo ser lineal.


## Par√°metro √≥ptimo de costo(**Punto d.**)

```{r,}

# Definimos un conjunto de valores de costo para probar
cost_values <- c(0.01, 0.1, 1, 10)


# Creamos un data frame para almacenar los resultados
results <- data.frame(Cost = double(), Accuracy = double(), stringsAsFactors = FALSE)

# Bucle para entrenar un modelo con cada valor de costo y evaluar su precisi√≥n mediante validaci√≥n cruzada
for (cost in cost_values) {
  set.seed(123) 
  # Entrenamos el modelo usando validaci√≥n cruzada
  train_control <- trainControl(method = "cv", number = 10) # 10-fold cross-validation
  svm_model <- train(Purchase ~ ., data = train_data, method = "svmLinear", trControl = train_control, tuneGrid = data.frame(C = cost))
  
  # Extraemos la precisi√≥n promedio de la validaci√≥n cruzada
  accuracy <- max(svm_model$results$Accuracy)
  
  # Almacenamos los resultados
  results <- rbind(results, data.frame(Cost = cost, Accuracy = accuracy))
}

# Graficamos los resultados
ggplot(results, aes(x = Cost, y = Accuracy)) +
  geom_line() + # L√≠nea que une los puntos
  geom_point() + # Los puntos de precisi√≥n para cada valor de costo
  scale_x_log10() + # Escala logar√≠tmica para el eje X
  labs(title = "Precisi√≥n de Validaci√≥n Cruzada vs Costo", x = "Costo (Log Scale)", y = "Precisi√≥n de Validaci√≥n Cruzada") +
  theme_minimal() # Tema minimalista para el gr√°fico


```

Primeros creamos un vector que tiene los costos que vamos a probar. Siguiendo la recomendaci√≥n en clase, elegimos costos con diferencias de 10^-1. Luego vamos a utilizar los costos: 0.01, 0.1, 1, 10. y calculamos el *Accuracy* mediante validaci√≥n cruzada(10 k folds), para esto guardamos nuestros valores para cada costo en una matriz y la gr√°ficamos para ver qu√© costo elegimos.

Aqui en la gr√°fica vemos algo interesante, nuestros valores √≥ptimos


```{r,}

# Probamos para valores de 0 a 1 con pasos de 0.10.
cost_values <- seq(0.10, 1, length.out = 10) 


# Creamos un data frame para almacenar los resultados
results <- data.frame(Cost = double(), Accuracy = double(), stringsAsFactors = FALSE)

# Bucle para entrenar un modelo con cada valor de costo y evaluar su precisi√≥n mediante validaci√≥n cruzada
for (cost in cost_values) {
  set.seed(123) 
  # Entrenamos el modelo usando validaci√≥n cruzada
  train_control <- trainControl(method = "cv", number = 10) # 10-fold cross-validation
  svm_model <- train(Purchase ~ ., data = train_data, method = "svmLinear", trControl = train_control, tuneGrid = data.frame(C = cost))
  
  # Extraemos la precisi√≥n promedio de la validaci√≥n cruzada
  accuracy <- max(svm_model$results$Accuracy)
  
  # Almacenamos los resultados
  results <- rbind(results, data.frame(Cost = cost, Accuracy = accuracy))
}

# Asumiendo que 'cost_values' es tu vector de valores de costo
cost_values <- seq(0.10, 1, length.out = 10)

# C√≥digo para graficar
ggplot(results, aes(x = Cost, y = Accuracy)) +
  geom_line() + # L√≠nea que une los puntos
  geom_point() + # Los puntos de precisi√≥n para cada valor de costo
  scale_x_log10(breaks = cost_values, labels = round(cost_values, 2)) + # Escala logar√≠tmica para el eje X con etiquetas personalizadas
  labs(title = "Precisi√≥n de Validaci√≥n Cruzada vs Costo", x = "Costo (Escala Logar√≠tmica)", y = "Precisi√≥n de Validaci√≥n Cruzada") +
  theme_minimal() # Tema minimalista para el gr√°fico

```

Aqui en la gr√°fica vemos que alcanzamos el valor optimo en 0.5,Luego vamos a utilizar este valor para el ejercicio.


## Valor √ìptimo(**Punto e.**)

```{r}
# Ajuste del modelo SVC(Purchase contra las dem√°s, y el kernel linear con el costo de 0.01)
svc_model_otimo <- svm(Purchase ~ ., data = train_data, kernel = "linear", cost = 0.5)


```

### Evaluaci√≥n Train(Costo √ìptimo)(**Punto e.**)

```{r,}
# Realizamos las predicciones en el conjunto de entrenamiento
pred_train <- predict(svc_model_otimo, train_data)

# Agregamos la matriz de confusi√≥n
confusion_matrix_train <- table(pred_train, train_data$Purchase)
print("Matriz de Confusi√≥n en los datos de entrenamiento:")
print(confusion_matrix_train)

# Calculamos la precisi√≥n
accuracy_train <- sum(pred_train == train_data$Purchase) / nrow(train_data)
print(paste("Precisi√≥n en los datos de entrenamiento:", accuracy_train))

# Calculamos la tasa de error de entrenamiento (1 - precisi√≥n)
error_rate_train <- 1 - accuracy_train
print(paste("Tasa de error en los datos de entrenamiento:", error_rate_train))

```

Observamos que hemos alcanzado un Accuracy de 0.83625, ligeramente superior al que calculamos en la primera parte(para los datos de entrenamiento).

### Evaluaci√≥n Test(Costo √ìptimo)(**Punto e.**)

```{r,}
# Realizamos las predicciones en el conjunto de prueba
pred_test <- predict(svc_model_otimo, test_data)

# Agregamos la matriz de confusi√≥n
confusion_matrix_test <- table(pred_test, test_data$Purchase)
print("Matriz de Confusi√≥n en los datos de prueba:")
print(confusion_matrix_test)

# Calculamos la precisi√≥n
accuracy_test <- sum(pred_test == test_data$Purchase) / nrow(test_data)
print(paste("Precisi√≥n en los datos de prueba:", accuracy_test))

# Calculamos la tasa de error de prueba (1 - precisi√≥n)
error_rate_test <- 1 - accuracy_test
print(paste("Tasa de error en los datos de prueba:", error_rate_test))


```
Alcanzamos una precisi√≥n de 0.837 para los datos de prueba. Tambi√©n es un poco mejor que la que calculamos utilizando un costo de 0.01.

## Kernel Radial(**Punto f.**)


```{r}

svc_model_Radial<- svm(Purchase ~ ., data = train_data, kernel = "radial", cost = 0.5)


# Realizamos las predicciones en el conjunto de entrenamiento
pred_train <- predict(svc_model_Radial, train_data)

# Agregamos la matriz de confusi√≥n
confusion_matrix_train <- table(pred_train, train_data$Purchase)
print("Matriz de Confusi√≥n en los datos de entrenamiento:")
print(confusion_matrix_train)

# Calculamos la precisi√≥n
accuracy_train <- sum(pred_train == train_data$Purchase) / nrow(train_data)
print(paste("Precisi√≥n en los datos de entrenamiento:", accuracy_train))

# Calculamos la tasa de error de entrenamiento (1 - precisi√≥n)
error_rate_train <- 1 - accuracy_train
print(paste("Tasa de error en los datos de entrenamiento:", error_rate_train))

# Realizamos las predicciones en el conjunto de prueba
pred_test <- predict(svc_model_Radial, test_data)

# Agregamos la matriz de confusi√≥n
confusion_matrix_test <- table(pred_test, test_data$Purchase)
print("Matriz de Confusi√≥n en los datos de prueba:")
print(confusion_matrix_test)

# Calculamos la precisi√≥n
accuracy_test <- sum(pred_test == test_data$Purchase) / nrow(test_data)
print(paste("Precisi√≥n en los datos de prueba:", accuracy_test))

# Calculamos la tasa de error de prueba (1 - precisi√≥n)
error_rate_test <- 1 - accuracy_test
print(paste("Tasa de error en los datos de prueba:", error_rate_test))

```

En el *Accuracy* de entrenamiento vemos que nuestro numero crece utilizando el kernel radial, llega a 85%. Sin embarbo en los datos de test tiene un valor de 80.37 que esta un poco por debajo de nuestro modelo lineal. Lo que indica que nuestro modelo puede estar sobreajustandose. 

## Kernel Polinomial de Grado 2(**Punto g.**)


```{r}

svc_model_Polinomial <- svm(Purchase ~ ., data = train_data, kernel = "polynomial", degree = 2, cost = 0.5)

# Realizamos las predicciones en el conjunto de entrenamiento
pred_train <- predict(svc_model_Polinomial, train_data)

# Agregamos la matriz de confusi√≥n
confusion_matrix_train <- table(pred_train, train_data$Purchase)
print("Matriz de Confusi√≥n en los datos de entrenamiento:")
print(confusion_matrix_train)

# Calculamos la precisi√≥n
accuracy_train <- sum(pred_train == train_data$Purchase) / nrow(train_data)
print(paste("Precisi√≥n en los datos de entrenamiento:", accuracy_train))

# Calculamos la tasa de error de entrenamiento (1 - precisi√≥n)
error_rate_train <- 1 - accuracy_train
print(paste("Tasa de error en los datos de entrenamiento:", error_rate_train))

# Realizamos las predicciones en el conjunto de prueba
pred_test <- predict(svc_model_Polinomial, test_data)



# Calculamos la precisi√≥n
accuracy_test <- sum(pred_test == test_data$Purchase) / nrow(test_data)
print(paste("Precisi√≥n en los datos de prueba:", accuracy_test))

# Calculamos la tasa de error de prueba (1 - precisi√≥n)
error_rate_test <- 1 - accuracy_test
print(paste("Tasa de error en los datos de prueba:", error_rate_test))

```

## **Punto h.**

En cuanto al Accuracy de nuestros datos de prueba, utilizando el soporte vectorial lineal alcanzamos un valor de 0.82. Para el Kernel Radial, alcanzamos un valor de 0.803, y para el kernel polinomial tenemos un valor de 0.77. Observamos que el valor m√°s alto se logra utilizando el kernel lineal con el valor √≥ptimo de 0.5.

Al comparar con los datos de entrenamiento, notamos que el kernel Radial es el que arroja el mejor resultado, alcanzando un 0.852 de precisi√≥n. Sin embargo, en los datos de prueba, esta precisi√≥n disminuye a 0.80. Esta diferencia podr√≠a indicar que el modelo est√° sobreajustado, ya que muestra un rendimiento m√°s alto en los datos de entrenamiento y menor en los de prueba